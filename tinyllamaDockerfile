# Use a lightweight base image like Alpine for a small footprint
FROM alpine:latest

# Install necessary packages for building and running llama.cpp
# We only need the base build tools and git
RUN apk add --no-cache \
    build-base \
    git \
    cmake \
    g++

# Set a working directory inside the container
WORKDIR /app

# Clone the llama.cpp repository
RUN git clone https://github.com/ggerganov/llama.cpp.git

# Change to the llama.cpp directory
WORKDIR /app/llama.cpp

# Download a TinyLlama GGUF model file
RUN wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_0.gguf

# --- Use CMake for the build process with the flag to disable CURL ---
# Create a build directory
RUN mkdir build
WORKDIR /app/llama.cpp/build

# Configure the build with CMake, turning off the CURL feature
RUN cmake .. -DLLAMA_CURL=OFF

# Build the project
RUN cmake --build .

# --- End of fix ---

# Expose a port if you plan to run a server
EXPOSE 8080

# Command to run when the container starts
CMD ["./bin/server", "-m", "/app/llama.cpp/tinyllama-1.1b-chat-v1.0.Q4_0.gguf", "-c", "2048", "--host", "0.0.0.0"]
