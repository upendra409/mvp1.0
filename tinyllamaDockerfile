# Use a lightweight base image like Alpine for a small footprint
FROM alpine:latest

# Install necessary packages for building and running llama.cpp
# Note: cmake is already included in the previous step, but we list it explicitly here
RUN apk add --no-cache \
    build-base \
    git \
    cmake \
    g++

# Set a working directory inside the container
WORKDIR /app

# Clone the llama.cpp repository
RUN git clone https://github.com/ggerganov/llama.cpp.git

# Change to the llama.cpp directory
WORKDIR /app/llama.cpp

# Download a TinyLlama GGUF model file
# We are using a 1.1B version, quantized to Q4_0
RUN wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_0.gguf

# --- FIX: Use CMake for the build process ---
# Create a build directory
RUN mkdir build
WORKDIR /app/llama.cpp/build

# Configure the build with CMake
RUN cmake ..

# Build the project
RUN cmake --build .

# --- End of FIX ---

# Expose a port if you plan to run a server
EXPOSE 8080

# Command to run when the container starts
# Note: The server executable is now in the 'build' directory
CMD ["./bin/server", "-m", "/app/llama.cpp/tinyllama-1.1b-chat-v1.0.Q4_0.gguf", "-c", "2048", "--host", "0.0.0.0"]
