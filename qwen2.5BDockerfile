# Use official vLLM OpenAI-compatible image
FROM vllm/vllm-openai:latest

# Set environment variables for Qwen 2.5 3B
ENV MODEL_NAME=Qwen/Qwen2.5-3B-Instruct
ENV TENSOR_PARALLEL_SIZE=1
ENV MAX_MODEL_LEN=4096
ENV DTYPE=float16
ENV HOST=0.0.0.0
ENV PORT=8000
ENV SERVED_MODEL_NAME=qwen2.5-3b

# CPU-specific optimizations
ENV OMP_NUM_THREADS=4
ENV MKL_NUM_THREADS=4
ENV TOKENIZERS_PARALLELISM=false

# Install additional dependencies if needed
RUN pip install --no-cache-dir transformers>=4.37.0

# Create cache directory for models
RUN mkdir -p /root/.cache/huggingface

# Set HuggingFace cache directory
ENV HF_HOME=/root/.cache/huggingface
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface

# Optional: Pre-download the model (uncomment if you want to bake model into image)
# RUN python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; \
#     AutoTokenizer.from_pretrained('${MODEL_NAME}', cache_dir='/root/.cache/huggingface'); \
#     AutoModelForCausalLM.from_pretrained('${MODEL_NAME}', cache_dir='/root/.cache/huggingface')"

# Expose the port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=15s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Start vLLM server with Qwen 2.5 3B
CMD python -m vllm.entrypoints.openai.api_server \
    --model ${MODEL_NAME} \
    --tensor-parallel-size ${TENSOR_PARALLEL_SIZE} \
    --max-model-len ${MAX_MODEL_LEN} \
    --dtype ${DTYPE} \
    --host ${HOST} \
    --port ${PORT} \
    --served-model-name ${SERVED_MODEL_NAME} \
    --device cpu \
    --disable-log-requests
