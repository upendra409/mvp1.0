# Use a lightweight base image like Alpine for a small footprint
FROM alpine:latest

# Install necessary packages for building and running llama.cpp
RUN apk add --no-cache \
    build-base \
    git \
    cmake \
    g++

# Set a working directory inside the container
WORKDIR /app

# Clone the llama.cpp repository
RUN git clone https://github.com/ggerganov/llama.cpp.git

# Change to the llama.cpp directory
WORKDIR /app/llama.cpp

# Download a TinyLlama GGUF model file
# We are using a 1.1B version, quantized to Q4_0
RUN wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_0.gguf

# Build the llama.cpp main executable
# The make command builds the core binaries
RUN make

# Expose a port if you plan to run a server
# For simple CLI usage, this isn't strictly necessary but is good practice
EXPOSE 8080

# Command to run when the container starts
# This starts the llama.cpp server and loads the downloaded model
CMD ["./server", "-m", "tinyllama-1.1b-chat-v1.0.Q4_0.gguf", "-c", "2048", "--host", "0.0.0.0"]
